{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TexProcessing.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nrjcs/mlip/blob/master/d1/TexProcessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRLp1jZ3UFzL",
        "colab_type": "text"
      },
      "source": [
        "# **NLTK**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MauM4-j0OOgr",
        "colab_type": "code",
        "outputId": "7f936e4e-dbf7-487b-fc34-ed999029b472",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        }
      },
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download(\"popular\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection u'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCaFdbjhUBGd",
        "colab_type": "text"
      },
      "source": [
        "## **Stopwords Removal, Word Tokenize and Sentence Tokenize**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrYjH4KYT9p4",
        "colab_type": "code",
        "outputId": "3240a526-4aff-464f-886c-4cdbdf475eba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize \n",
        "from nltk import sent_tokenize \n",
        "\n",
        "example_sent = \"The city of Bihta welcomes you for a course which starts from today\"\n",
        "\n",
        "stop_words = set(stopwords.words('english')) \n",
        "\n",
        "tk = word_tokenize(example_sent) \n",
        "\n",
        "print example_sent\n",
        "print \"tokens\", tk\n",
        "\n",
        "filtered_sentence = [w for w in tk if not w in stop_words] \n",
        "print \"----combined way----\"\n",
        "print filtered_sentence\n",
        "\n",
        "print \"-----the longer route----\"\n",
        "\n",
        "filtered_sentence = [] \n",
        "\n",
        "for w in tk: \n",
        "\tif w not in stop_words: \n",
        "\t\tfiltered_sentence.append(w) \n",
        "print(filtered_sentence) \n",
        "\n",
        "ex=\"It is quite cold today. You might consider a jacket!\"\n",
        "ex1=sent_tokenize(ex)\n",
        "print \"----sentences----\"\n",
        "print(ex1)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The city of Bihta welcomes you for a course which starts from today\n",
            "tokens ['The', 'city', 'of', 'Bihta', 'welcomes', 'you', 'for', 'a', 'course', 'which', 'starts', 'from', 'today']\n",
            "----combined way----\n",
            "['The', 'city', 'Bihta', 'welcomes', 'course', 'starts', 'today']\n",
            "-----the longer route----\n",
            "['The', 'city', 'Bihta', 'welcomes', 'course', 'starts', 'today']\n",
            "----sentences----\n",
            "['It is quite cold today.', 'You might consider a jacket!']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWPSfrUsUK9w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "ps = PorterStemmer()\n",
        "#csv.field_size_limit(sys.maxsize)\n",
        "\n",
        "reload(sys)\n",
        "sys.setdefaultencoding('utf-8')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJkuDmEjX-wA",
        "colab_type": "text"
      },
      "source": [
        "## **Lower case & upper case**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYF_UYolYFPp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print \"Bread Butter\".lower()\n",
        "print \"heritage\".upper()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wb6h0zYXVJ19",
        "colab_type": "text"
      },
      "source": [
        "## **stemming**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "au9C3qJ3UwPH",
        "colab_type": "code",
        "outputId": "3bccf731-69a8-4456-9012-7c82d335cef7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "ps.stem(\"eating\")\n",
        "str(ps.stem(\"eating\") )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'eat'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xilcV9NlMLYO",
        "colab_type": "text"
      },
      "source": [
        "## **Installing PyDictionary**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htXi9h3TWCVX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install PyDictionary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNuZ2WkvVYqr",
        "colab_type": "code",
        "outputId": "73cb434f-13b0-42ae-d22e-1eb1397a38f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from PyDictionary import PyDictionary\n",
        "dictionary = PyDictionary()\n",
        "\n",
        "print (dictionary.meaning(\"light\"))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{u'Adjective': ['of comparatively little physical weight or density', '(used of color', 'of the military or industry; using (or being', 'not great in degree or quantity or number', 'psychologically light; especially free from sadness or troubles', 'characterized by or emitting light', '(used of vowels or syllables', 'easily assimilated in the alimentary canal; not rich or heavily seasoned', '(used of soil', '(of sound or color', 'moving easily and quickly; nimble', 'demanding little effort; not burdensome', 'of little intensity or power or force', '(physics, chemistry', 'weak and likely to lose consciousness', 'very thin and insubstantial', 'marked by temperance in indulgence', 'less than the correct or legal or full amount often deliberately so', 'having little importance', 'intended primarily as entertainment; not serious or profound', 'silly or trivial', 'designed for ease of movement or to carry little weight', 'having relatively few calories', 'or lite', 'or light', '(of sleep', 'casual and unrestrained in sexual behavior', 'or light'], u'Verb': ['make lighter or brighter', 'begin to smoke', 'to come to rest, settle', 'cause to start burning; subject to fire or great heat', 'fall to somebody by assignment or lot', 'alight from (a horse', 'start or maintain a fire in'], u'Noun': ['(physics', 'any device serving as a source of illumination', 'a particular perspective or aspect of a situation', 'the quality of being luminous; emitting or reflecting light', 'an illuminated area', 'a condition of spiritual awareness; divine illumination', 'the visual effect of illumination on objects or scenes as created in pictures', 'a person regarded very fondly', 'having abundant light or illumination', 'mental understanding as an enlightening experience', 'merriment expressed by a brightness or gleam or animation of countenance', 'public awareness', 'a divine presence believed by Quakers to enlighten and guide the soul', 'a visual warning signal', 'a device for lighting or igniting fuel or charges or fires'], u'Adverb': ['with few burdens']}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKszcw53heg1",
        "colab_type": "text"
      },
      "source": [
        "# Synonyms and Antonym"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CntY4HDYhixZ",
        "colab_type": "code",
        "outputId": "1d7fcd30-4746-45d4-80b0-972208dde128",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "from nltk.corpus import wordnet \n",
        "\n",
        "syns = wordnet.synsets(\"light\") \n",
        "print syns\n",
        "\n",
        "synonyms = [] \n",
        "antonyms = [] \n",
        "  \n",
        "for syn in wordnet.synsets(\"light\"): \n",
        "    for l in syn.lemmas(): \n",
        "        synonyms.append(l.name()) \n",
        "        if l.antonyms(): \n",
        "            antonyms.append(l.antonyms()[0].name()) \n",
        "  \n",
        "print(set(synonyms)) \n",
        "print(set(antonyms)) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Synset('light.n.01'), Synset('light.n.02'), Synset('light.n.03'), Synset('luminosity.n.01'), Synset('light.n.05'), Synset('light.n.06'), Synset('light.n.07'), Synset('light.n.08'), Synset('light.n.09'), Synset('light.n.10'), Synset('sparkle.n.01'), Synset('light.n.12'), Synset('inner_light.n.01'), Synset('light.n.14'), Synset('lighter.n.02'), Synset('light.v.01'), Synset('light_up.v.05'), Synset('alight.v.01'), Synset('ignite.v.01'), Synset('fall.v.20'), Synset('unhorse.v.01'), Synset('light.a.01'), Synset('light.a.02'), Synset('light.a.03'), Synset('light.a.04'), Synset('light.a.05'), Synset('light.a.06'), Synset('unaccented.s.02'), Synset('light.s.08'), Synset('light.s.09'), Synset('clean.s.03'), Synset('light.s.11'), Synset('light.s.12'), Synset('light.a.13'), Synset('light.a.14'), Synset('faint.s.04'), Synset('light.s.16'), Synset('abstemious.s.02'), Synset('light.s.18'), Synset('light.s.19'), Synset('light.s.20'), Synset('idle.s.04'), Synset('light.s.22'), Synset('light.s.23'), Synset('light.s.24'), Synset('easy.s.10'), Synset('lightly.r.02')]\n",
            "set([u'ignite', u'perch', u'sluttish', u'luminance', u'illuminate', u'Inner_Light', u'Christ_Within', u'alight', u'unaccented', u'get_off', u'wakeful', u'light-colored', u'visible_light', u'illumination', u'spark', u'wanton', u'luminousness', u'abstemious', u'lightly', u'visible_radiation', u'faint', u'short', u'Light_Within', u'sparkle', u'light-headed', u'easy', u'scant', u'swooning', u'promiscuous', u'unhorse', u'clean', u'light_source', u'Light', u'low-cal', u'loose', u'weak', u'twinkle', u'ignitor', u'illumine', u'lighting', u'lighter', u'fall', u'brightness_level', u'lite', u'lightness', u'tripping', u'get_down', u'lightheaded', u'brightness', u'light', u'clear', u'igniter', u'idle', u'dismount', u'lightsome', u'illume', u'luminosity', u'calorie-free', u'light_up', u'fire_up', u'unclouded'])\n",
            "set([u'dark', u'heavy', u'extinguish'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQyHsitBMtZ7",
        "colab_type": "text"
      },
      "source": [
        "## **Substring Replacement**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1ksW9gTMheU",
        "colab_type": "code",
        "outputId": "3e3c7ddd-c7c0-4f96-efa2-69eae325d865",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "string=\"This iz my #world of $interest\"\n",
        "\n",
        "replacestring=\"z\"\n",
        "replacewith=\"s\"\n",
        "string=string.replace(replacestring, replacewith)\n",
        "print string\n",
        "\n",
        "print string.replace(\"#\", \"\") "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is my #world of $interest\n",
            "This is my world of $interest\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToKi5UOBUTre",
        "colab_type": "text"
      },
      "source": [
        "## **Substring Occurences in a String**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUtIs1K5i-l0",
        "colab_type": "code",
        "outputId": "efc7e570-acca-4811-9e3d-4c58b0edcf13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "import re\n",
        "import sys\n",
        "\n",
        "substring=\"well\"\n",
        "string=\"we are doing well but how well are you\"\n",
        "\n",
        "result = string.find(substring) \n",
        "result1= string.find(substring,14)\n",
        "\n",
        "print result\n",
        "print result1\n",
        "\n",
        "startList=[m.start() for m in re.finditer(substring, string)]\n",
        "print startList\n",
        "\n",
        "overlappingList=[i for i in range(len(string)) if string.startswith(substring, i)]\n",
        "print overlappingList"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13\n",
            "26\n",
            "[13, 26]\n",
            "[13, 26]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94TACEqL6UZ5",
        "colab_type": "text"
      },
      "source": [
        "## **Assignment**\n",
        "\n",
        "\n",
        "1. Write python code to read a file  named \"demonetization_tweets_handled\" and do the following operations on the tweets read from the file.\n",
        "\n",
        "1. Read all the tweets from the file and store it in a list.\n",
        "2. Tokenize each of the tweet.\n",
        "3. Remove stop words, punctuations, urls, usernames(that start with @), from the corresponding tokens of each of the tweet.\n",
        "4. Calculate and print cosine similarity between each pair of tweets.\n",
        "5. Find the most similar pair of tweets that are not duplicates.\n",
        "6. Replace wherever \"demonetization\" occurs into it's capitalized form in the list created at 1.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}